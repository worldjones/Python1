{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HTML Refresher\n",
    "This part is based on chapter 11 of *Automate the Boring Stuff with Python* by Al Sweigart\n",
    "\n",
    "HTML files are plain text files containing *tags*, which are words enclosed in angle brackets. Tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags.\n",
    "\n",
    "There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the `<a>` tag encloses text that should be a link.\n",
    "\n",
    "Some elements have an `id` attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << EOF > data/example.html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Hello!</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Hello World!</h1>\n",
    "You are extremely welcome!<br>\n",
    "<br>\n",
    "The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\">Lecture Notes</a>.<br>\n",
    "<div>\n",
    "<p>paragraph 1</p>\n",
    "<p>and paragraph 2: <span id=\"span01\">This is span 1</span id=\"span02\"><span id=\"span03\">Second span element</span>\n",
    "<span class=\"red_border\">Here is the third span</span>\n",
    "</p>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html>\r\n",
      "<head>\r\n",
      "<title>Hello!</title>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "<h1>Hello World!</h1>\r\n",
      "You are extremely welcome!<br>\r\n",
      "<br>\r\n",
      "The <a href=\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\">Lecture Notes</a>.<br>\r\n",
      "<div>\r\n",
      "<p>paragraph 1</p>\r\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span id=\"span02\"><span id=\"span03\">Second span element</span>\r\n",
      "<span class=\"red_border\">Here is the third span</span>\r\n",
      "</p>\r\n",
      "</div>\r\n",
      "</body>\r\n",
      "</html>\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# View a Page's HTML Sources\n",
    "\n",
    "Here, I will only describe how to use Firefox' development features.\n",
    "\n",
    "To view a page's sources right click on it and choose **View page source** which opens a new tab with the HTML sources.\n",
    "\n",
    "<img src=\"images/view_source_small.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Firefox, you can bring up the Web Developer Tools Inspector by pressing `CTRL-SHIFT-C` on Windows and Linux or by `CMD-OPTION-C` on OS X.\n",
    "\n",
    "<img src=\"images/inspector_small.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a module for parsing and extracting information from HTML sources. The module’s name is bs4. In case it is not already installed on your machine:\n",
    "- install it with `pip install beautifulsoup4`. While beautifulsoup4 is the name used for installation, \n",
    "- to import BeautifulSoup you have to use `import bs4`.\n",
    "\n",
    "According to its documentation (https://www.crummy.com/software/BeautifulSoup/) *\"Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Local HTML File\n",
    "\n",
    "- The `bs4.BeautifulSoup()` function needs to be called with a string containing the HTML it will parse. \n",
    "- The `bs4.BeautifulSoup()` function returns is a `BeautifulSoup` object.\n",
    "\n",
    "You can load a local HTML file and pass a file object to `bs4.BeautifulSoup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Hello!\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   Hello World!\n",
      "  </h1>\n",
      "  You are extremely welcome!\n",
      "  <br/>\n",
      "  <br/>\n",
      "  The\n",
      "  <a href='\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\"'>\n",
      "   Lecture Notes\n",
      "  </a>\n",
      "  .\n",
      "  <br/>\n",
      "  <div>\n",
      "   <p>\n",
      "    paragraph 1\n",
      "   </p>\n",
      "   <p>\n",
      "    and paragraph 2:\n",
      "    <span id=\"span01\">\n",
      "     This is span 1\n",
      "    </span>\n",
      "    <span id=\"span03\">\n",
      "     Second span element\n",
      "    </span>\n",
      "    <span class=\"red_border\">\n",
      "     Here is the third span\n",
      "    </span>\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html)\n",
    "print(type(soup))\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a BeautifulSoup Object from a Remote HTML File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"https://github.githubassets.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://github-cloud.s3.amazonaws.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://user-images.githubusercontent.com/\" rel=\"dns-prefetch\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/frameworks-434290b33c0a12fe2ebc85c17723b585.css\" integrity=\"sha512-Q0KQszwKEv4uvIXBdyO1hb0uTXdXFpSWGMeWBg1wKa2SANz62ydQRiVsF8qWt2JQ/W3kF+tqdDAsCobPLghBiw==\" media=\"all\" rel=\"stylesheet\">\n",
      "   <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/site-f205ca43654fa7224c4c585e80759ce5.css\" integrity=\"sha512-8gXKQ2VPpyJMTFhegHWc5WtBtMaDSTarMVgtu4dzRZh9SamHEP1Q4s//0xcsmg5Q1lbf37EIkiJFxWVFgsmUAQ==\" media=\"all\" rel=\"stylesheet\">\n",
      "    <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/behaviors-18fb5d2bfe9317452157d423d6942119.css\" integrity=\"sha512-GPtdK/6TF0UhV9Qj1pQhGUseKYv9xo5n8Sq1jjcV9o4Rbl8GyzB1ubRWlwU7FXX8RrWBUHPaPNT0iOH24dF4ZA==\" media=\"all\" rel=\"stylesheet\">\n",
      "     <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/github-b520f569f058537c08c17c980c693d5c.css\" integrity=\"sha512-tSD1afBYU3wIwXyYDGk9XEHpzh9KNUpufc0Eu+s+8cfiP6TVZUUxEiZci/allIvQfThbqi/CJapWsCf0etmWyA==\" media=\"all\" rel=\"stylesheet\">\n",
      "      <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-8K2vvwbW+6H27Nad5\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get('https://github.com/datsoftlyngby/dat4sem2020spring-python')\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(soup.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding an Element with the `select()` Method\n",
    "\n",
    "You can retrieve HTML elements from a `BeautifulSoup` object by calling the `select()` method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: They specify a pattern to look for, in this case, in HTML pages instead of general text strings.\n",
    "\n",
    "Common CSS selector patterns include:\n",
    "\n",
    "  * `soup.select('div')` ... selects all elements named `<div>`\n",
    "  * `soup.select('#lecturer')`  ... selects the element with an id attribute of author\n",
    "  * `soup.select('.notice')` ... selects all elements that use a CSS class attribute named notice\n",
    "  * `soup.select('div span')` ... selects all elements named `<span>` that are within an element named `<div>`\n",
    "  * `soup.select('div > span')` ... selects all elements named `<span>` that are directly within an element named `<div>`, with no other element in between\n",
    "  * `soup.select('input[name]')` ... selects all elements named `<input>` that have a name attribute with any value\n",
    "  * `soup.select('input[type=\"button\"]')` ... selects all elements named `<input>` that have an attribute named type with value button\n",
    "  \n",
    "See more in the documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: return type of select() <class 'list'>\n",
      "2: length of the returned list 1\n",
      "3: type of elements in the list <class 'bs4.element.Tag'>\n",
      "4: get text from the element \n",
      "Hello World!\n",
      "You are extremely welcome!\n",
      "5: string representation of an element:  <body>\n",
      "<h1>Hello World!</h1>\n",
      "You are extremely welcome!<br/>\n",
      "<br/>\n",
      "The <a href='\\\"https://github.com/datsoftlyngby/dat4sem2019spring-python-materials\\\"'>Lecture Notes</a>.<br/>\n",
      "<div>\n",
      "<p>paragraph 1</p>\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span><span id=\"span03\">Second span element</span>\n",
      "<span class=\"red_border\">Here is the third span</span>\n",
      "</p>\n",
      "</div>\n",
      "</body>\n",
      "6: the attributes of the element:  {}\n",
      "{'id': 'span01'}\n",
      "{'id': 'span03'}\n",
      "{'class': ['red_border']}\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "\n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "\n",
    "elems = soup.select('body')\n",
    "\n",
    "#print(soup.prettify())\n",
    "print('1: return type of select()',type(elems))\n",
    "print('2: length of the returned list',len(elems))\n",
    "print('3: type of elements in the list',type(elems[0]))\n",
    "print('4: get text from the element',elems[0].getText()[:40])\n",
    "print('5: string representation of an element: ',str(elems[0]))\n",
    "print('6: the attributes of the element: ',elems[0].attrs)\n",
    "elements = soup.select('div > p > span')\n",
    "for element in elements:\n",
    "    print(element.attrs)\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>paragraph 1</p>\n",
      "paragraph 1\n",
      "------------\n",
      "<p>and paragraph 2: <span id=\"span01\">This is span 1</span><span id=\"span03\">Second span element</span>\n",
      "<span class=\"red_border\">Here is the third span</span>\n",
      "</p>\n",
      "and paragraph 2: This is span 1Second span element\n",
      "Here is the third span\n",
      "\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "p_elems = soup.select('p')\n",
    "\n",
    "for el in p_elems:\n",
    "    print(str(el))\n",
    "    print(el.getText())\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting Data from an Element’s Attributes\n",
    "\n",
    "The `get()` method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lecturer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'id': 'lecturer'}.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span id=\"span01\">This is span 1</span>\n",
      "span01\n",
      "True\n",
      "{'id': 'span01'}\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "with open('./data/example.html') as f:\n",
    "    example_html = f.read()\n",
    "    \n",
    "soup = bs4.BeautifulSoup(example_html, 'html.parser')\n",
    "# soup.find_all?\n",
    "span_elem = soup.select('span')[0]\n",
    "print(str(span_elem))\n",
    "print(span_elem.get('id'))\n",
    "print(span_elem.get('some_nonexistent_addr') == None)\n",
    "print(span_elem.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the difference between the `select` and the `find`/`find_all` functions?\n",
    "\n",
    "You are not the first ones wondering about this... See:\n",
    "https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x#38033910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Scraping Events from a Page\n",
    "\n",
    "\n",
    "Ususally, you will use web scraping to collect information, which you cannot gather otherwise. \n",
    "For example, let's imagine we want to do some statistics about:\n",
    "- concerts in Copenhagen, \n",
    "- their start times and \n",
    "- their door prices.\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage www.kultunaut.dk, \n",
    "\n",
    "The website lists all possible events in Denmark. \n",
    "Concerts in Copenhagen are for example accessible here: \n",
    "- http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&periode=&Genre=Musik\n",
    "\n",
    "**OBS** Many web pages are not built to support high traffic or they exlicitely discourage automatic access. Keep this in mind when writing your scraping tool.\n",
    "- from time import sleep\n",
    "- sleep(3) # sleep 3 seconds\n",
    "\n",
    "\n",
    "Considering our example:\n",
    "- we have to first figure out how many events there are at all. \n",
    "- We need this information, as events are given paginated, i.e., twenty events per page.\n",
    "- The link given above only returns the link to the first page with the first twenty events. \n",
    "- Out of the total amount of events we can generate the URLs for the subsequent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kultunaut_url = 'https://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&ArrStartdato=16%2F3+2021&ArrSlutdato=15%2F4+2021&Genre=Musik'\n",
    "kultunaut_url2 = 'https://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?showmap=&Area=Kbh.+og+Frederiksberg&ArrStartdato=16%2F3+2021&ArrSlutdato=15%2F4+2021&Genre=Musik&Startnr={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of events: 120\n",
      "Div with class arr-genre <div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Music\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Åben kirke</strong></h3>\n",
      "</div>\n",
      "\n",
      "\n",
      "[<strong>Åben kirke</strong>, <strong>Morgenmusik</strong>, <strong>Åben kirke</strong>, <strong>Åbent kirkerum i Frederiksholm kirke</strong>, <strong>Aftenmusik i fastetiden</strong>, <strong>Mariakirken er åben</strong>, <strong>UDSAT: Kammerkoncert med Tinne Albrectsen</strong>, <strong>Aflyst! DR Vokalensemblet</strong>, <strong>Anders Bergcrantz</strong>, <strong>Jazzy Bossa Trio - Online Concert</strong>, <strong>Thor Farlov (DK)</strong>, <strong>AFLYST - Familiekoncert med Musikflyveren</strong>]\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(kultunaut_url)\n",
    "r.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "res = soup.select('.result-count > strong:nth-child(2)')[0].text\n",
    "no_of_events = res\n",
    "print('no of events: {}'.format(no_of_events))\n",
    "\n",
    "\n",
    "elems = soup.findAll('div',{'class':'arr-genre'})\n",
    "print('Div with class arr-genre',elems[0])\n",
    "print();print()\n",
    "# annother notation\n",
    "elems = soup.select('div[class=arr-genre] > h3 > strong')\n",
    "print(elems)\n",
    "#print(b_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the browser inspector pane:\n",
    "\n",
    "<img src=\"images/inspect_element.png\" width=\"500\">\n",
    "\n",
    "We can see that the desired element is hiding in a structure like: a b-tag inside a h3-tag inside a td-tag or:\n",
    "- `('td h3 b')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Børnekor\n",
      "Juniorkor 3. 5. klasse\n",
      "Husum & Husumvold Kirkers Børnekor\n",
      "Spirekor\n",
      "Husum & Husumvold Kirkers Juniorkor\n",
      "Københavns Kammerkor øver\n",
      "Henschel-kvartetten - Kammermusikforeningen af 1887\n",
      "Backstage Open - en musikalsk aften, hvor alt kan ske\n",
      "Cæcilie Norby - Få Billetter\n",
      "Juke Jones (US)\n",
      "Stig Møller & Peter Ingemann Band\n",
      "Familiekoncert på Kirkebjerg Skole\n"
     ]
    }
   ],
   "source": [
    "# use select with css-selectors rather than find_all\n",
    "import bs4\n",
    "import requests\n",
    "html = requests.get(kultunaut_url)\n",
    "txt = html.text\n",
    "soup = bs4.BeautifulSoup(txt, 'html.parser')\n",
    "events = soup.select('div[class=arr-genre] > h3 > strong')\n",
    "for e in events:\n",
    "    print(e.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <!-- Initalize title and data source variables -->\n",
      " <head>\n",
      "  <!--\n",
      "\n",
      "------------------------\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "https://analytics.usa.gov/data/live/all-pages-realtime.csv\n",
      "https://analytics.usa.gov/data/live/all-domains-30-days.csv\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\n",
      "https://support.google.com/analytics/answer/2763052?hl=en\n",
      "https://analytics.usa.gov/data/live/second-level-domains.csv\n",
      "https://analytics.usa.gov/data/live/sites.csv\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "https://github.com/GSA/analytics.usa.gov/issues\n",
      "https://github.com/GSA/analytics.usa.gov\n",
      "https://github.com/18F/analytics-reporter\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://cloud.gov/\n"
     ]
    }
   ],
   "source": [
    "# Get all the links in a document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "gov = requests.get('https://analytics.usa.gov')\n",
    "soup = BeautifulSoup(gov.text, 'lxml')\n",
    "print(soup.prettify()[:100])\n",
    "print('------------------------')\n",
    "for link in soup.find_all('a'):\n",
    "    if not link.get('href').startswith('https'):\n",
    "        continue\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we can scrape the events per page. Observe, that now, our `base_url` http://www.kultunaut.dk/perl/arrlist/type-nynaut/UK?Startnr={}&showmap=&Area=Kbh.%20og%20Frederiksberg&periode=&Genre=Musik& has a placeholder for the paginated results (`Startnr=`).\n",
    "\n",
    "Consequently, we scrape each page separately, see the function on the next slide: `scrape_events_per_page`. From examining the page's source code, we know that events are all given as table entries with a corresponding header. We iterate over each of the table cells and extract the strings for dates and prices if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:06,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Music\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Åben kirke</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Tingbjerg Kirke holder åben kirke tirsdage 14.00-16.00 og onsdage 10.00-12.00 i februar. Kom og tænd et lys og lad tankerne flyve. Der vil være stille musik i kirken - enten fra orglet eller anlægget.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Tues 16 Mar 2021 2 pm, <b>Tingbjerg Kirke</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:01<00:05,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Orgelmatiné</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Aeji Choi (organist ved Sct Thomas Kirke, Frb.) spiller en halv times program på domkirkens store orgel.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Sat 20 Mar 2021 12 am, <b>Vor Frue Kirke - Københavns Domkirke</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:02<00:04,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Sangaften, Kammermusik</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Clara Cecilie Thomsen, sopran og Christian Vestergaard, klaver synger og spiller værker af P.Heise, R. Strauss, Brahms og Sibelius.\n",
      "\n",
      "  Organizer Kammermusikforeningen af 1911.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Wed 24 Mar 2021 7.30 pm, <b>Garnisonskirken</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:02<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Rock/Pop\n",
      "        </span>\n",
      "<h3><strong>Piano Days Copenhagen - Live stream direkte fra KoncertKirken</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>PROGRAM: Christian Rønn // Ea Wim // Eva Sidén &amp; Christian Rønn // Jomi Massage // Kamil Piotrowicz // Makiko Hirabayashi // Sune T.B. Nielsen.\n",
      "\n",
      "  Organizer Piano Days Copenhagen.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Fri 26 Mar 2021 and Sat 27 Mar 2021 7 pm, <b>KoncertKirken (Blågårds Kirke)</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:03<00:03,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Music\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Aflyst! Ars Nova - Requiem</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>John Frandsens Requiem i en kammerversion for kor og orgel, skrevet til Ars Nova Copenhagen. Ars Nova Copenhagen. Kristian Krogsøe, orgel Paul Hillier, dirigent Entré: 150 kr.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Thur 1 Apr 2021 2 pm  - 3.30 pm, <b>Trinitatis Kirke</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:04<00:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Music\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Niels Hausgaard - Bides Hestene?</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Niels hausgaard klar med nyt 2021 SHOW: Bides Hestene? (skitse til et show 2021). Hausgaards rytme er rystet. \"Plejer\" er i sandhed død.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Wed 7 Apr 2021 7.30 pm, <b>Tivoli - Glassalen</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:04<00:02,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Music\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Barbara Moleko</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Barbara Moleko rykkes fra den 29.01.21 til den 09.04.21.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Fri 9 Apr 2021 6 pm, <b>Copenhagen Admiral Hotel</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:05<00:01,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Children\n",
      "          \n",
      "        </span>\n",
      "<h3><strong>Børnekoncert: Jorden rundt med Malene Kjærgård (flyttet fra 12. dec. 2020)</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Vi har fået fingrene i orkestret Jorden Rundt, der netop har vundet en Danish Music Award for årets bedste børnekoncert.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Sat 10 Apr 2021 2 pm  - 3 pm, <b>Kulturhuset Pilegården</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Jazz\n",
      "        </span>\n",
      "<h3><strong>Svaneborg Kardyb &amp; Gustaf Ljunggre</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Denne Monten-aften byder på to duoer, der forener det akustiske og elektroniske i minimalistiske lydlandskaber. Den danske jazzduo Svaneborg Kardyb er et af de nyeste skud på stammen i dansk jazz.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Sun 11 Apr 2021 8 pm, <b>Jazzhus Montmartre</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"arr-info\">\n",
      "<div class=\"arr-genre\">\n",
      "<span class=\"genre_cat notranslate\">\n",
      "          Classical\n",
      "        </span>\n",
      "<h3><strong>Kammermusik, Nightingale Kvartetten</strong></h3>\n",
      "</div>\n",
      "<div class=\"arr-description\"><span>Nightingale Kvartetten, der består af Gunvor Sihm og Josefine Dalsgaard, violiner, Marie Louise Broholt Jensen, bratsch og Louisa Schwab, cello, spiller værker af Rud Langaard, Shostakovich og Mendelssohn.\n",
      "\n",
      "  Organizer Kammermusikforeningen af 1911.\n",
      "\n",
      "</span>\n",
      "</div>\n",
      "<div class=\"kult-month-day\">\n",
      "<time>Wed 14 Apr 2021 7.30 pm, <b>Garnisonskirken</b></time>\n",
      "<div class=\"arrow_right\"><img alt=\"arrow right\" src=\"https://www.kultunaut.dk/images/nynaut22/np-arrow-right.png\"/></div>\n",
      "</div>\n",
      "</div>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def scrape_events_per_page(url):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        A list of tuples of strings holding title, place, date, and price\n",
    "        for concerts in Copenhagen scraped from Kulturnaut.dk\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    event_cells = soup.find_all('div', {'class' : 'arr-info'})\n",
    "    #print('size',len(event_cells))\n",
    "    print(event_cells[0])\n",
    "    scraped_events_per_page = []\n",
    "    \n",
    "    for event_cell in event_cells:\n",
    "        try:\n",
    "            title = event_cell.select('h3 > strong')[0].text\n",
    "            rest = event_cell.find('time').text.split(',')\n",
    "            try:\n",
    "                place = rest[1]\n",
    "            except:\n",
    "                place = ''\n",
    "            try:\n",
    "                date = rest[0]\n",
    "            except:\n",
    "                date = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        scraped_events_per_page.append((title, place, date))\n",
    "        \n",
    "    return scraped_events_per_page\n",
    "\n",
    "\n",
    "scraped_events = []\n",
    "indexes = list(range(1, int(no_of_events), 12))\n",
    "indexes[0] = 0\n",
    "\n",
    "\n",
    "for idx in tqdm(indexes):\n",
    "    scrape_url = kultunaut_url2.format(idx)\n",
    "    #print(scrape_url)\n",
    "    scraped_events += scrape_events_per_page(scrape_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Åben kirke', ' Tingbjerg Kirke', 'Tues 16 Mar 2021 2 pm'),\n",
       " ('Morgenmusik', ' Sankt Markus Kirke', 'Wed 17 Mar 2021 10 am'),\n",
       " ('Åben kirke', ' Tingbjerg Kirke', 'Wed 17 Mar 2021 10 am'),\n",
       " ('Åbent kirkerum i Frederiksholm kirke',\n",
       "  ' Frederiksholm Kirke',\n",
       "  'Wed 17 Mar 2021 10 am'),\n",
       " ('Aftenmusik i fastetiden', ' Vor Frelsers Kirke', 'Wed 17 Mar 2021 5.30 pm'),\n",
       " ('Mariakirken er åben', ' Mariakirken', 'Thur 18 Mar 2021 7 pm'),\n",
       " ('UDSAT: Kammerkoncert med Tinne Albrectsen',\n",
       "  ' Grundtvigs Kirke',\n",
       "  'Thur 18 Mar 2021 7.30 pm'),\n",
       " ('Aflyst! DR Vokalensemblet',\n",
       "  ' Trinitatis Kirke',\n",
       "  'Thur 18 Mar 2021 7.30 pm  - 9.30 pm'),\n",
       " ('Anders Bergcrantz', ' Jazzhus Montmartre', 'Thur 18 Mar 2021 8 pm'),\n",
       " ('Jazzy Bossa Trio - Online Concert',\n",
       "  ' Caféen skjolds plads',\n",
       "  'Fri 19 Mar 2021 7.00 pm'),\n",
       " ('Thor Farlov (DK)', ' VEGA', 'Fri 19 Mar 2021 8 pm'),\n",
       " ('AFLYST - Familiekoncert med Musikflyveren',\n",
       "  ' Børnekulturstedet Karens Minde',\n",
       "  'Sat 20 Mar 2021 10 am  & 1 pm'),\n",
       " ('Orgelmatiné',\n",
       "  ' Vor Frue Kirke - Københavns Domkirke',\n",
       "  'Sat 20 Mar 2021 12 am'),\n",
       " ('Koncert med Ask Yggdrasil', ' Metronomen', 'Sat 20 Mar 2021 7.30 pm'),\n",
       " ('Morten Lund Special Quartet',\n",
       "  ' Jazzhus Montmartre',\n",
       "  'Sat 20 Mar 2021 8 pm'),\n",
       " ('Emil Stabil (dk) - Flyttet', ' VEGA', 'Sat 20 Mar 2021 8 pm'),\n",
       " ('AFLYST - Familiekoncert med Musikflyveren',\n",
       "  ' Kulturhuset Islands Brygge',\n",
       "  'Sun 21 Mar 2021 10 am  & 1 pm'),\n",
       " ('Åben kirke', ' Tingbjerg Kirke', 'Tues 23 Mar 2021 2 pm'),\n",
       " ('The Nordic Piano Trio i Metronomen',\n",
       "  ' Metronomen',\n",
       "  'Tues 23 Mar 2021 8 pm'),\n",
       " ('Low Roar', ' Hotel Cecil', 'Tues 23 Mar 2021 9 pm')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_events[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Extract Dates and Prices from Strings.\n",
    "\n",
    "Remember, the raw data, which we extracted from the web pages is all of type `str`. To do statistics about possible correlation of start times and entry fees, we need to convert the corresponding tuple fields into datetimes and integers respectively.\n",
    "\n",
    "\n",
    "Since dates given on the web do not necessarily conform to standardized time formats, we can apply the `dateparser` (https://pypi.python.org/pypi/dateparser) module, which tries to parse arbitrary strings into datetimes.\n",
    "\n",
    "You can read more about the module and its capabilities https://dateparser.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#pip install dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:01<00:00, 118.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2021, 3, 16, 14, 0), datetime.datetime(2021, 3, 17, 10, 0), datetime.datetime(2021, 3, 17, 10, 0), datetime.datetime(2021, 3, 17, 10, 0), datetime.datetime(2021, 3, 17, 17, 30), datetime.datetime(2021, 3, 19, 19, 0), datetime.datetime(2021, 3, 19, 20, 0), datetime.datetime(2021, 3, 20, 10, 0), datetime.datetime(2021, 3, 20, 19, 30), datetime.datetime(2021, 3, 20, 20, 0), datetime.datetime(2021, 3, 20, 20, 0), datetime.datetime(2021, 3, 21, 10, 0), datetime.datetime(2021, 3, 23, 14, 0), datetime.datetime(2021, 3, 23, 20, 0), datetime.datetime(2021, 3, 23, 21, 0), datetime.datetime(2021, 3, 24, 10, 0), datetime.datetime(2021, 3, 24, 10, 0), datetime.datetime(2021, 3, 24, 10, 0), datetime.datetime(2021, 3, 24, 17, 30), datetime.datetime(2021, 3, 24, 19, 30), datetime.datetime(2021, 3, 26, 16, 30), datetime.datetime(2021, 3, 26, 20, 0), datetime.datetime(2021, 3, 27, 10, 0), datetime.datetime(2021, 3, 27, 20, 0), datetime.datetime(2021, 3, 30, 14, 0), datetime.datetime(2021, 3, 30, 18, 0), datetime.datetime(2021, 3, 30, 19, 0), datetime.datetime(2021, 3, 30, 19, 30), datetime.datetime(2021, 3, 31, 10, 0), datetime.datetime(2021, 3, 31, 10, 0), datetime.datetime(2021, 3, 31, 10, 0), datetime.datetime(2021, 3, 31, 21, 0), datetime.datetime(2021, 3, 31, 21, 0), datetime.datetime(2021, 4, 2, 20, 0), datetime.datetime(2021, 4, 2, 23, 0), datetime.datetime(2021, 4, 3, 19, 30), datetime.datetime(2021, 4, 3, 21, 30), datetime.datetime(2021, 4, 4, 15, 0), datetime.datetime(2021, 4, 6, 19, 0), datetime.datetime(2021, 4, 6, 19, 30), datetime.datetime(2021, 4, 6, 20, 0), datetime.datetime(2021, 4, 7, 19, 30), datetime.datetime(2021, 4, 7, 20, 0), datetime.datetime(2021, 4, 7, 20, 0), datetime.datetime(2021, 4, 9, 18, 0), datetime.datetime(2021, 4, 9, 19, 0), datetime.datetime(2021, 4, 9, 19, 0), datetime.datetime(2021, 4, 9, 19, 0), datetime.datetime(2021, 4, 9, 19, 30), datetime.datetime(2021, 4, 9, 20, 0), datetime.datetime(2021, 4, 9, 20, 0), datetime.datetime(2021, 4, 9, 20, 0), datetime.datetime(2021, 4, 9, 21, 0), datetime.datetime(2021, 4, 9, 21, 30), datetime.datetime(2021, 4, 10, 10, 0), datetime.datetime(2021, 4, 10, 14, 0), datetime.datetime(2021, 4, 10, 16, 0), datetime.datetime(2021, 4, 10, 17, 30), datetime.datetime(2021, 4, 10, 19, 0), datetime.datetime(2021, 4, 10, 19, 0), datetime.datetime(2021, 4, 10, 19, 30), datetime.datetime(2021, 4, 10, 20, 0), datetime.datetime(2021, 4, 10, 20, 30), datetime.datetime(2021, 4, 11, 15, 0), datetime.datetime(2021, 4, 11, 16, 0), datetime.datetime(2021, 4, 11, 19, 0), datetime.datetime(2021, 4, 11, 20, 0), datetime.datetime(2021, 4, 11, 20, 0), datetime.datetime(2021, 4, 11, 20, 0), datetime.datetime(2021, 4, 12, 20, 0), datetime.datetime(2021, 4, 12, 20, 0), datetime.datetime(2021, 4, 12, 21, 0), datetime.datetime(2021, 4, 13, 19, 0), datetime.datetime(2021, 4, 13, 19, 0), datetime.datetime(2021, 4, 13, 20, 0), datetime.datetime(2021, 4, 13, 20, 0), datetime.datetime(2021, 4, 13, 20, 0), datetime.datetime(2021, 4, 14, 16, 0), datetime.datetime(2021, 4, 14, 18, 0), datetime.datetime(2021, 4, 14, 19, 30), datetime.datetime(2021, 4, 14, 20, 0), datetime.datetime(2021, 4, 14, 20, 0), datetime.datetime(2021, 4, 14, 20, 0), datetime.datetime(2021, 4, 14, 21, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "from dateparser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_dates_and_prices(scraped_events):\n",
    "    \"\"\"\n",
    "    NO LONGER WORKS WELL WITH KULTUNAUT website after they changes layout and hid the prices behind a js function.\n",
    "    Cleanup the data. Get price as integer and date as date.\n",
    "    \n",
    "    returns:\n",
    "        A two-element tuple with a datetime representing the start \n",
    "        time of an event and an integer representing the price in Dkk.\n",
    "    \"\"\"\n",
    "\n",
    "    price_regexp = r\"(?P<price>\\d+)\" #initial ? is a lookbehind. r() r is for raw text, P<some pattern> is to give a pattern name to refer to. \\d is numeric digit, + is for 1 or more.\n",
    "\n",
    "    data_points = []\n",
    "    three_at_night = datetime.now().replace(hour=3, minute=0, second=0, microsecond=0).time()\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str, price_str = event_data\n",
    "        \n",
    "        if 'Free admission' in price_str:\n",
    "            price = 0\n",
    "        else:\n",
    "            m = re.search(price_regexp, price_str) # m is the Match object returned from re.search (might be None)\n",
    "            try:\n",
    "                price = int(m.group('price')) # if price can be converted to int then we do it else return 0.\n",
    "            except:\n",
    "                price = 0\n",
    "\n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date and date.time() > three_at_night:\n",
    "            data_points.append((date, price))\n",
    "            \n",
    "    return data_points\n",
    "\n",
    "def get_dates(scraped_events):\n",
    "    \"\"\"\n",
    "    Cleanup the data. Get date as date.\n",
    "    \n",
    "    returns:\n",
    "        A datetime representing the start \n",
    "        time of an event.\n",
    "    \"\"\"\n",
    "    three_at_night = datetime.now().replace(hour=3, minute=0, second=0, microsecond=0).time()\n",
    "    dates = []\n",
    "    for event_data in tqdm(scraped_events):\n",
    "        title_str, place_str, date_str = event_data\n",
    "        \n",
    "        date_str = date_str.strip().strip('.')\n",
    "        if '&' in date_str:\n",
    "            date_str = date_str.split('&')[0]\n",
    "        if '-' in date_str:\n",
    "            date_str = date_str.split('-')[0]\n",
    "        if '.' in date_str:\n",
    "            date_str = date_str.replace('.', ':')\n",
    "        \n",
    "        date = parse(date_str)\n",
    "        if date and date.time() > three_at_night:\n",
    "            dates.append(date)\n",
    "    return dates\n",
    "\n",
    "dates = get_dates(scraped_events)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2020, 10, 5, 22, 0),\n",
       " datetime.datetime(2020, 10, 6, 16, 30),\n",
       " datetime.datetime(2020, 10, 5, 15, 0),\n",
       " datetime.datetime(2020, 10, 5, 15, 20),\n",
       " datetime.datetime(2020, 10, 5, 16, 0),\n",
       " datetime.datetime(2020, 10, 5, 16, 0),\n",
       " datetime.datetime(2020, 10, 5, 17, 15),\n",
       " datetime.datetime(2020, 10, 5, 18, 0),\n",
       " datetime.datetime(2020, 10, 5, 19, 30),\n",
       " datetime.datetime(2020, 10, 5, 20, 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping Images from a Page\n",
    "\n",
    "In the following code you will use Beautiful Soup to extract all links to images, which are in `img` tags on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTYE6YsI0YEwP8cRXlXtw0HciPzNzJZ-_X6CK6N9hykVTt8YtPMPT0EApe59k4&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTrO7PBf2eVJp3pMg6oLsyuLoG-XEUaokc9obrLCX1V8EkUWapLeGU4eH2rP4&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTIZS2fOqtnc-XoUAFGcFjVN5bPJUWpw58NMOXSy3HEifUgJwxYNv4_2MvaufQ&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQs9nN8QlRO1Jd7pvF9SDhz16f333eoYV9MF-xweI5T8SOJDQfGfazYudGidA&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSKl9k2Hu8uKp5FBuxBCztZrnd4kvA0-TIxMKlcZVopJ7HQPa8BMONioQf4bg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSZ4InI7il13xnevzZvakcdJVRpxH9B4cikeAG4cEDOFzoJ__27zKGWNAHF8g&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeHlNBWWWjLJBkbmrV51AAVaAxSYMONttxZn6TtXI4DnRTgnKefTCxomPU5A&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHa6M9PaqE9sQZU5NpF3w0TiPGisqJqGeIDVFtYDMjtqru0Y-EuW0ttO1EEIk&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSusHaf3K2Iyv34J9ji1yKaXCmxVVZPou2PyeOXWpW0qaqkz7HzqHFxB430zg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTAJjBcaK5VY81yh2gumpS2I8EYYsBRorck9MSuCD33RmgUKzjKA1Wsw6YUVK4&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSxhwqjOpI_Kg4Ol6cSSUi_3P3CnjiyrJfeSK1zS-d7-oEnDGi4T3dIrJ8pCFU&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQVL5KJ0UN_Ak_2kQB6q1slJad-9sUXUd0lpjXSI91fKQMscq2CZ0SduiTNU-s&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSF2QndIXveHRmFOrIDUEQ1-Q-Cd_-ZLl2cd0GvdfAl9YyhmaRfNXKO1XQL8R0&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSiJeJeRJyMr_ynan01V9rAnJQqoTqdTaU3UxMBX9WvILvTYnKiBeesI3d8YSY&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQQHM8QBP78-Tc3yP4V0NR6dtywOirrcyo2WtPSRrsbgHpHCPvIl8lU2f69qg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQyiPeR32OlDrizODKxcb1i80Rn56Zw3rEpZzdBokpDtG3Zx5vbhfmGyY0ZdxU&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPxPtG7SUI6pmMUkEjZnz7UExQ1-u2xkXUVdiImHTu_Q_NMc6RXlAuDzSSyw&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQVk7GoC5SoXBMcTP74_KDCzFXo0m14D5l7xiI7PIwMlRrHGkUc6fanZ4TsRvg&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQBYgLSQjNz5VBc1BMaoC1Rh7Oiru1a7yxBITkmtDLakJ02vq7qEaIjqtdljpE&s', 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRsL_rlbefQ71HAWVB9PRtlIA0ZIjdbwU2dv7IPr5XPkGKm5OSzNgJjyLzssw&s']\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "\n",
    "def collect_img_links(url):\n",
    "    \"\"\"based on a url returns a list of image links contained in the requested page\"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #print(soup.select('img'))\n",
    "    return [img.get('src') for img in soup.select('img') \n",
    "            if img.get('src') and img.get('src').startswith('http')]\n",
    "\n",
    "\n",
    "def download_imgs(links, out_folder=\"./data/test/\"):\n",
    "    \"\"\"download all images from a list of image links. \n",
    "    Requires a folder named: test to be there\"\"\"\n",
    "    img_no = 0\n",
    "    for l in links:\n",
    "        img_no += 1\n",
    "        r = requests.get(l, stream=True)\n",
    "        with open(out_folder+'img'+str(img_no)+'.jpg', 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)     \n",
    "        \n",
    "links = collect_img_links('https://www.google.dk/search?site=&tbm=isch&source=hp&biw=1163&bih=812&q=minions&oq=minions')\n",
    "print(links)\n",
    "download_imgs(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Exercise: Writing a Simple Web Crawler\n",
    "\n",
    "1. Write a simple web crawler that can capture all links in a document (like: https://www.cphbusiness.dk/). And all links of the linked document - so 2 levels of documents will be scraped. Use threads if helpfull\n",
    "\n",
    "\n",
    "In case a page returns a status code, which is not `200` we just disregard this page. See https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for more detailes on the various HTTP status codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_links(from_url, for_depth, all_links={}):\n",
    "    # TODO: Implement code for websraper\n",
    "    # return dict(key=url, value=list of outgoing urls)\n",
    "    pass\n",
    "\n",
    "\n",
    "start_url = 'https://www.version2.dk/artikel/google-deepmind-vi-oeger-sikkerheden-mod-misbrug-sundhedsdata-1074452'\n",
    "\n",
    "link_dict = scrape_links(from_url=start_url, for_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The web crawler that you wrote above is perhaps not the most performant. If you are interested in more web scraping and application of crawlers have a look at the `scrapy` module (https://scrapy.org)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
